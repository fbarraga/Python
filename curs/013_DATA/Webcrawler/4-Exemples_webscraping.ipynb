{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Extracción del texto de un discurso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de la librería pandas\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"button | flex\" data-ctn-subscription=\"\" data-link-am=\"https://elpais.com/suscripciones/america/#/campaign#?prod=SUSDIGCRARTAM&amp;o=cerradoam&amp;prm=signwall_articulo_landing_el-pais-america&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\" data-link-ar=\"https://elpais.com/suscripciones/america/#/campaign#?prod=SUSDIGCRARTAR&amp;o=cerrado&amp;prm=signwall_articulo_landing_el-pais&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\" data-link-ch=\"https://elpais.com/suscripciones/america/#/campaign#?prod=SUSDIGCRARTCHILE&amp;o=cerrado&amp;prm=signwall_articulo_landing_el-pais&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\" data-link-co=\"https://elpais.com/suscripciones/america/#/campaign#?prod=SUSDIGCRARTCOL&amp;o=cerrado&amp;prm=signwall_articulo_landing_el-pais&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\" data-link-mx=\"https://elpais.com/suscripciones/america/#/campaign#?prod=SUSDIGCRARTMX&amp;o=cerradomx&amp;prm=signwall_articulo_landing_el-pais-mexico&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\" href=\"https://elpais.com/suscripciones/#/campaign#?prod=SUSDIGCRART&amp;o=cerrado&amp;prm=signwall_articulo_landing_el-pais&amp;backURL=https%3A%2F%2Felpais.com%2Finternacional%2F2018%2F01%2F31%2Factualidad%2F1517387619_036241.html\">O suscríbete para leer sin límites</a>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"Tag\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m         a\u001b[39m=\u001b[39m tag\u001b[39m.\u001b[39mcontents[\u001b[39m0\u001b[39m]                          \u001b[39m# Extraigo el contenido de la etiqueta\u001b[39;00m\n\u001b[0;32m     12\u001b[0m         \u001b[39mprint\u001b[39m(a)\n\u001b[1;32m---> 13\u001b[0m         discurso \u001b[39m=\u001b[39m discurso \u001b[39m+\u001b[39;49m  a                    \u001b[39m# Voy concatenando cada contenido a la variable discurso\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#print(discurso)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m contadores \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m() \u001b[39m# Creo un diccionario para almacenar las palabras y contar las veces que aparece\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"Tag\") to str"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://elpais.com/internacional/2018/01/31/actualidad/1517387619_036241.html'\n",
    "html = urllib.request.urlopen(url)\n",
    "soup2 = BeautifulSoup(html)\n",
    "#print(soup2)\n",
    "tags = soup2('p')                                   # Extraigo las etiquetas de tipo <p>\n",
    "discurso = ''                                       # Aquí guardo el discurso, inicialmente vacío\n",
    "for tag in tags:\n",
    "    if (len(tag.attrs)) == 0:                       # Con esta condición filtro solo el texto que me interesa\n",
    "        a= tag.contents[0]                          # Extraigo el contenido de la etiqueta\n",
    "        print(a)\n",
    "        discurso = discurso +  a                    # Voy concatenando cada contenido a la variable discurso\n",
    "        \n",
    "#print(discurso)\n",
    "\n",
    "contadores = dict() # Creo un diccionario para almacenar las palabras y contar las veces que aparece\n",
    "discurso = discurso.replace(',','').replace('.','').replace(':','').replace('?','').replace('(','').replace(')','') # Elimino signos de (puntos, comas, etc.)\n",
    "palabras = discurso.lower().split() # Paso todas las palabras a minúsculas (lower) y las corto palabra a palabra (split)\n",
    "for palabra in palabras:\n",
    "    if len(palabra)>3: # Añado al diccionario solo las palabras con más de 3 letras\n",
    "        contadores[palabra] = contadores.get(palabra,0) + 1 \n",
    "        \n",
    "        \n",
    "import pandas as pd\n",
    "# Guardo las palabras en un dataframe ordenados de mayor a menor por el contador\n",
    "pd.DataFrame(list(contadores.items()), columns=['palabra', 'contador']).sort_values('contador', ascending=False )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Extracción de valores de un fondo de inversión "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer datos de una web tenemos que analizar el código fuente. Si inspeccionamos el valor del fondo vemos que tiene esta etiqueta: \n",
    "```html\n",
    "<td class=\"line text\">EUR 15.15</td>\n",
    "```\n",
    "\n",
    "Por lo tanto, voy a utilizar BeautifulSoup para extraer las etiquetas `\"td\"` con `class = \"line text\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849.0\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "#iShares Dev Rl Ett Idx (IE) Instl Acc €\n",
    "participaciones = 50 # Estas son mis participaciones en el fondo de inversión\n",
    "url= 'http://www.morningstarfunds.ie/ie/funds/snapshot/snapshot.aspx?id=F00000PJME'\n",
    "html=urllib.request.urlopen(url)\n",
    "soup=BeautifulSoup(html)\n",
    "tags = soup.find_all(\"td\",class_=\"line text\") # Extraigo las etiquetas \n",
    "valor = float(tags[0].contents[0].replace('EUR\\xa0','')) # Me quedo únicamente con el valor numérico\n",
    "total = participaciones * valor\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Web scraping de laliga.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraemos los hashtags de los próximos encuentros  \n",
    "<img src='partidos2.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#LevanteAtleti', '#RealValladolidValencia', '#SevillaFCAthletic', '#GetafeVillarreal', '#EspanyolRealSociedad', '#HuescaLeganés', '#AlavésGirona', '#RealMadridRealBetis', '#EibarBarça', '#PlayOffLaLiga123', '#PlayOffLaLiga123']\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.laliga.es'\n",
    "html = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "tags = soup.find_all(\"a\",class_=\"hashtag big2\")\n",
    "\n",
    "hashtags = list()\n",
    "\n",
    "for tag in tags:\n",
    "    hashtags.append(tag.contents[0])\n",
    "print(hashtags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
